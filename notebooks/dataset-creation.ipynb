{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Set options to display all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation\n",
    "* For each client that has gone through the workbook, we have already gotten their plans and then matched those plans with the plans from the Ideon dataset. This has been done manually. Creating the dataset will be one of the more difficult tasks.\n",
    "* For each client, we want to go get all the available plans, create a dataframe, match only the matched plans in a dataset, and then keep doing that for all clients.\n",
    "* After (or maybe during) we then need to create the false match case, where we still need to store all of the plans for a given client and then create a false match in the dataset.\n",
    "* For each plan should we have every plan being a false match? How should we create the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in small group plans actuals to begin to build dataset\n",
    "small_group_actuals_file_path = '~/match-plans/data/small_group_plans_actuals.csv'\n",
    "small_group_actuals_df = pd.read_csv(small_group_actuals_file_path)\n",
    "small_group_actuals_df['plan_effective_start_date'] = pd.to_datetime(small_group_actuals_df['plan_effective_start_date'])\n",
    "small_group_actuals_df.rename(columns={'plan_id':'id','carrier_name':'census_carrier_name'},inplace=True)\n",
    "\n",
    "\n",
    "# prep the data to be able to query and iterate through\n",
    "grouped = small_group_actuals_df.groupby(['client_name', 'zip_code', 'plan_effective_start_date'])\n",
    "\n",
    "# Initialize list to store client objects\n",
    "clients = []\n",
    "\n",
    "# Loop through each group\n",
    "for (client_name, zip_code, start_date), group in grouped:\n",
    "    client = {}\n",
    "    client['client_name'] = client_name\n",
    "    client['zip_code'] = zip_code\n",
    "    client['plan_effective_start_date'] = start_date\n",
    "\n",
    "    # Extract year and quarter\n",
    "    year = start_date.year\n",
    "    quarter = (start_date.month - 1) // 3 + 1\n",
    "    \n",
    "    client['year'] = year\n",
    "    client['quarter'] = f'Q{quarter}'\n",
    "    client['plans'] = group.drop(columns=['client_name', 'zip_code', 'plan_effective_start_date']).to_dict(orient='records')\n",
    "    clients.append(client)\n",
    "\n",
    "print(clients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery Client to Query Dataset\n",
    "GOOGLE_APPLICATION_CREDENTIALS = '/Users/kieranshaw/.config/gcloud/application_default_credentials.json'\n",
    "\n",
    "# initialize the client and saved dataframe\n",
    "bq_client = bigquery.Client()\n",
    "df = pd.DataFrame()\n",
    "start_time = time.time()\n",
    "\n",
    "for client in tqdm(clients, desc='Processing clients'):\n",
    "    query_start_time = time.time()\n",
    "\n",
    "    query = f\"\"\"\n",
    "        WITH SelectedZip AS (\n",
    "            SELECT * \n",
    "            FROM airbyte_ideon.zip_counties\n",
    "            WHERE zip_code_id = '{client['zip_code']}'\n",
    "            AND _ab_source_file_url LIKE '%CA/{client['year']}/{client['quarter']}%'\n",
    "        )\n",
    "        SELECT\n",
    "            p.*\n",
    "        FROM \n",
    "            SelectedZip zc\n",
    "        JOIN \n",
    "            airbyte_ideon.plan_counties pc \n",
    "            ON zc.county_id = pc.county_id\n",
    "            AND pc._ab_source_file_url LIKE '%CA/{client['year']}/{client['quarter']}%'\n",
    "        JOIN \n",
    "            airbyte_ideon.plans p \n",
    "            ON p.id = pc.plan_id\n",
    "            AND p._ab_source_file_url LIKE '%CA/{client['year']}/{client['quarter']}%'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run the query\n",
    "    query_job = bq_client.query(query)\n",
    "\n",
    "    # Fetch the results\n",
    "    results = query_job.result()\n",
    "\n",
    "    # Convert the results to a pandas DataFrame\n",
    "    query_df = results.to_dataframe()\n",
    "\n",
    "    # Create a DataFrame from the 'plans' key in the client object\n",
    "    client_plans_df = pd.DataFrame(client['plans'])\n",
    "\n",
    "    # Add columns for the client info\n",
    "    client_plans_df['client_name'] = client['client_name']\n",
    "    client_plans_df['zip_code'] = client['zip_code']\n",
    "    client_plans_df['plan_effective_start_date'] = client['plan_effective_start_date']\n",
    "\n",
    "    # Merge the results with the client info\n",
    "    match_df = pd.merge(client_plans_df, query_df, on='id')\n",
    "    match_df['is_match'] = 1\n",
    "\n",
    "    # NON MATCHING PLANS FAKE DATA\n",
    "    non_matching_plans = query_df[~query_df['id'].isin(client_plans_df['id'])]\n",
    "\n",
    "    # Create a DataFrame for non-matching plans with client info\n",
    "    non_matching_df = non_matching_plans.copy()\n",
    "    non_matching_df['client_name'] = client['client_name']\n",
    "    non_matching_df['zip_code'] = client['zip_code']\n",
    "    non_matching_df['plan_effective_start_date'] = client['plan_effective_start_date']\n",
    "    non_matching_df['is_match'] = 0  # Label as non-match\n",
    "    non_matching_df['plan_admin_name'] = np.NaN\n",
    "    non_matching_df['census_carrier_name'] = np.NaN\n",
    "\n",
    "    # Add random plan_admin_name and carrier_name from the client's set\n",
    "    client_plan_admin_names = set(client_plans_df['plan_admin_name'].dropna())\n",
    "    client_carrier_names = set(client_plans_df['census_carrier_name'].dropna())\n",
    "    non_matching_df['plan_admin_name'] = non_matching_df.apply(lambda row: random.choice(list(client_plan_admin_names)) if pd.isnull(row['plan_admin_name']) else row['plan_admin_name'], axis=1)\n",
    "    non_matching_df['census_carrier_name'] = non_matching_df.apply(lambda row: random.choice(list(client_carrier_names)) if pd.isnull(row['census_carrier_name']) else row['census_carrier_name'], axis=1)\n",
    "\n",
    "    final_df = pd.concat([match_df, non_matching_df], ignore_index=True)\n",
    "    df = pd.concat([df, final_df], ignore_index=True)\n",
    "    query_end_time = time.time()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Processed {len(clients)} clients in {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('~/match-plans/data/small_group_dataset.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f44cbdfb2efbc0cea1434bb138a03e1afe958e8794e0d0448957ca4693084e2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
